---
title: "Component response rate variation drives stability in large complex systems"
author: "Brad Duthie"
bibliography: references.bib
output:
  html_document: default
  pdf_document:
    fig_caption: yes
  word_document:
    fig_caption: yes
    pandoc_args:
    - --csl
    - evolution.csl
    reference_docx: docx_template.docx
header-includes:
- \usepackage{amsmath}
- \usepackage{natbib}
- \usepackage{lineno}
- \usepackage[utf8]{inputenc}
- \linenumbers
- \bibliographystyle{amnatnat}
linestretch: 1
link-citations: yes
linkcolor: blue
csl: nature.csl
biblio-style: apalike
---

```{r, echo = FALSE}
source(file = "R/sim_mat.R");
source(file = "R/plot_figs.R");
```

********************************************************************************

**The stability of a complex system generally decreases with increasing system size, as is demonstrated by random matrix theory [@May1972; @Allesina2012]. This counter-intuitive result, first shown by May [@May1972], is broadly relevant for understanding the dynamics and persistence of systems such as ecological [@May1972; @Allesina2012], neurological [@Gray2008; @Gray2009], biochemical [@Rosenfeld2009; @MacArthur2010] and socio-economic [@Haldane2011; @Suweis2014; @Bardoscia2017] networks. Much attention has especially been given to the stability of ecological communities such as food webs or mutualist networks, with recent work investigating how different community structures affect stability [@Allesina2012; @Mougi2012; @Allesina2015a; @Gao2016; @Grilli2017; @Patel2018]. But more broadly, stabilising mechanisms in complex systems remain under-developed, and the effect of variation in the response rate of individual system components remains an open problem [@Allesina2015]. Here I show that when components of a complex system respond to system dynamics at different rates ($\boldsymbol{\gamma}$), the potential for system stability is markedly increased. Stability increases due to the clustering of some eigenvalues toward the centre of eigenvalue distributions despite the destabilising effect of higher variation among interaction strengths ($\boldsymbol{\sigma^{2}}$). This effect of variation in $\boldsymbol{\gamma}$ becomes increasingly important as system size increases, to the extent that the largest stable complex systems would otherwise be unstable if not for $\boldsymbol{Var(\gamma)}$. My results therefore reveal a previously unconsidered driver of system stability that is likely to be pervasive across all complex systems. Future research in complex systems should therefore account for the varying response rates of individual system components when assessing whole system stability.** 

In 1972, May [@May1972] first demonstrated that randomly assembled systems of sufficient complexity are almost inevitably unstable given infinitesimally small perturbations. Complexity in this case is defined by the size of the system (i.e., the number of interacting components; $S$), its inter-connectivity (i.e., the probability that one component will affect another; $C$), and the variance of interaction strengths ($\sigma^{2}$)[@Allesina2012]. May's finding that the probability of local stability falls to near zero given a sufficiently high threshold of $\sigma\sqrt{SC}$ has profound consequences across multiple disciplines, raising the question of how complex systems in, e.g., ecology[@Mougi2012; @Allesina2012; @Allesina2015; @Grilli2017] or banking [@May2008; @Haldane2011; @Bardoscia2017] are predicted to persist or change.

Randomly assembled complex systems can be represented as large square matrices ($M$) with $S$ components (e.g., species[@Allesina2012] or banks[@Haldane2011]). One element of such a matrix $M_{ij}$ defines how component $j$ affects component $i$ in the system at a point of equilibrium[@Allesina2012]. Off-diagonal elements ($i \neq j$) therefore define interactions between components, while diagonal elements ($i = j$) define component self-regulation (e.g., carrying capacity in ecological communities). Traditionally, values of off-diagonal elements are assigned non-zero values with a probability $C$, which are sampled from a distribution with variance $\sigma^{2}$; diagonal elements are set to -1[@May1972; @Allesina2012; @Allesina2015]. Local system stability is assessed using eigenanalysis, with the system being stable if the real parts of all eigenvalues ($\lambda$) of $M$ are negative ($\max\left(\Re(\lambda)\right) < 0$)[@May1972; @Allesina2012]. In a large system (high $S$), eigenvalues are distributed uniformly[@Tao2010] within a circle centred at $\Re = -1$ (the mean value of diagonal elements) and $\Im = 0$, with a radius of $\sigma\sqrt{SC}$[@May1972; @Allesina2012; @Allesina2015] (Figs 1a and 2a). Local stability of randomly assembled systems therefore becomes increasingly unlikely as $S$, $C$, and $\sigma^{2}$ increase.

The above stability criterion assumes that individual components respond to perturbations of the system at the same rate ($\gamma$), but this is highly unlikely in any complex system. In ecological communities, for example, the rate at which population density changes following perturbation will depend on the generation time of individuals, which might vary by orders of magnitude among species. Species with short generation times will respond quickly (high $\gamma$) to perturbations relative to species with long generation times (low $\gamma$). Similarly, the speed at which individual banks respond to perturbations in financial networks, or individuals or institutions respond to perturbations in complex social networks, is likely to vary. The effect of such variance has not been investigated in complex systems theory. Intuitively, variation in $\gamma$ might be expected to decrease system stability by introducing a new source of variation into the system and thereby increasing $\sigma$. Here I show why, despite higher $\sigma$, complex systems in which $\gamma$ varies actually tend to be more stable, especially when $S$ is high.  

```{r echo = FALSE}
S     <- 200;
C     <- 0.05;
sigma <- 0.4;
pr_st <- read.csv(file = "sim_results/bi_gamma/bi_pr_st.csv");
pr_st <- pr_st[,-1];
```

Rows in $M$ define how a given component $i$ is affected by other components of the system, meaning that the rate of component response time can be modelled by multiplying all row elements by a scalar value $\gamma_{i}$[@Patel2018]. The distribution of $\gamma$ over $S$ components thereby models the distribution of component response rates. An instructive example compares one $M$ where $\gamma_{i} = 1$ for all $i$ in $S$ to the same $M$ when half of $\gamma_{i} = 1.95$ and half of $\gamma_{i} = 0.05$. This models one system in which $\gamma$ is invariant and one in which $\gamma$ varies, but systems are otherwise identical (note $E[\gamma_{i}] = 1$ in both cases). I assume $S = 200$, $C = 0.05$, and $\sigma = 0.4$; diagonal elements are set to $-1$ and non-zero off-diagonal elements are drawn from $\mathcal{N}(0, \sigma^{2})$. Rows are then multiplied by $y_{i}$ to generate $M$. When $\gamma_{i} = 1$, eigenvalues of $M$ are distributed uniformly within a circle centred at ($-1, 0$) with a radius of `r round(sigma*sqrt(S*C), digits = 3)` (Fig. 1a). Hence, the real components of eigenvalues are highly unlikely to all be negative when all $\gamma_{i} = 1$. But when $\gamma_{i}$ values are separated into two groups, eigenvalues are no longer uniformly distributed (Fig. 1b). Instead, two distinct clusters of eigenvalues appear (grey circles in Fig. 1b), one centred at ($-1.95, 0$) and the other centred at ($-0.05, 0$). The former has a large radius, but the real components have shifted to the left (in comparison to when $\gamma = 1$) and all $\Re({\lambda}) < 0$. The latter cluster has real components that have shifted to the right, but has a smaller radius. Overall, for 1 million randomly assembled $M$, this division between slow and fast component response rates results in more stable systems: `r sum(pr_st[,2])` stable given $\gamma = 1$ versus `r sum(pr_st[,2])` stable given $\gamma = \{1.95, 0.5\}$.

```{r echo = FALSE}
S     <- 1000;
C     <- 1;
sigma <- 0.4;
```

Higher stability in systems with variation in $\gamma$ can be observed by sampling $\gamma_{i}$ values from various distributions. I now focus on a uniform distribution where $\gamma \sim \mathcal{U}(0, 2)$ (see Supporting Information for other distributions, which give similar results). As with the case of $\gamma = \{1.95, 0.5\}$ (Fig. 1b), $E[\gamma] = 1$ when $\gamma \sim \mathcal{U}(0, 2)$, allowing comparison of $M$ before and after variation in component response rate. Figure 2 shows a comparison of eigenvalue distributions given $S = 1000$, $C = 1$, and $\sigma = 0.4$. As expected [@Tao2010], when $\gamma = 1$, eigenvalues are distributed uniformly in a circle centred at ($-1, 0$) with a radius of $\sigma\sqrt{SC} =$ `r round(sigma*sqrt(S*C), digits = 3)`. Uniform variation in $\gamma$ leads to a non-uniform distribution of eigenvalues, some of which are clustered tightly around the centre of the distribution, but others of which are spread outside the former radius of `r round(sigma*sqrt(S*C), digits = 3)` (red circle Fig 2b). This larger radius occurs because the addition of $Var(\gamma)$ increases the realised $\sigma$ of $M$. The clustering and spreading of eigenvalues introduced by $Var(\gamma)$ can destabilise previously stable systems or stabilise systems that are otherwise unstable. But where systems are otherwise too complex to be stable given $\gamma = 1$, the effect of $Var(\gamma)$ can often lead to stability above May's[@May1972; @Allesina2012] threshold of $\sigma\sqrt{SC} > 1$.

```{r, echo = FALSE}
dat <- read.csv(file = "sim_results/C_1/random_all.csv");
dat <- dat[,-1];
```

To investigate the effect of $Var(\gamma)$ on system stability, I simulated random $M$ matrices at $\sigma = 0.4$ and $C = 1$ across $S$ ranging from $2-32$ (see Supporting Information for different values of $\sigma$ and $C$). One million $M$ were simulated for each $S$, and the stability of $M$ was assessed given $\gamma = 1$ versus $\gamma \sim \mathcal{U}(0, 2)$ (note that under these conditions, $\sigma\sqrt{SC} = 1$ given $S = 25$ when $\gamma = 1$). I found that number of stable random systems was consistently higher given $Var(\gamma)$ than when $\gamma = 1$ (Fig. 3), and that the difference between the probabilities of observing a stable system increased with an increase in $S$; i.e., the potential for $Var(\gamma)$ to drive stability increased with system complexity. For the highest values of $S$, nearly all systems that were stable given $Var(\gamma)$ would not have been given $\gamma = 1$, and the maximum observed $S$ for which a system was stable was $31$ given $Var(\gamma)$ versus $27$ given $\gamma = 1$ (see Supporting Information for full results). This suggests that the stability of large systems might be dependent upon variation in the response rate of their individual components, meaning that factors such as generation time (in ecological networks), transaction speed (in economic networks), or communication speed (in social networks) needs to be considered when investigating the stability of complex systems.



<!--- Remember in Fig 3 that the line is showing the A1s that are stable *due to* var(gamma) -- sometimes A0s were stable, but they were destabilised by var(gamma) --->

<!---

Part of the answer might be that communities aren't actually random ....


2. Go back to @May1972 and explain the basic idea of random matrices as representing communities. 

===== ACTUALLY, use paragraph three to shift to considering component time scales not having been done (skip 3, sort of).

3. A paragraph with some broad explanation of the idea of matrix stability, along the lines of @Allesina2012. This will effectively explain the idea of random matrix theory to the reader. Explain the general conclusion that stability decreases whenever $\sigma\sqrt{SC} > 1$. Note how the original ecological result has been broadened for different types of networks (e.g., nestedness, predator-prey, mutualist, etc.), 

4. But none of these studies have considered that different components operate on different time scales (but see [e.g. @Patel2018]). Then explain that this, intuitively, should decrease stability because it introduces another source of variation and therefore increases $\sigma$. But then explain that it also changes the distribution of eignvalues, bringing them towards the centre of the plot (figure 1). I show that this actually increases the stability of complex systems.

5. A paragraph introducing, mathematically, how I am adding varying rates of component response -- multipling each row $i$ by a scalar $\gamma_{i}$. First this will be done with some simple function giving $Var(\gamma_{i})$, then in a more structured way such that random components that are more likely to be positively influenced by other components having faster or slower rates (higher or lower $\gamma_{i}$) of response. A sentence or two is needed to suggest what this might mean biologically (e.g., generalist predators have longer generation times than their prey). 

6. A paragraph explaining the results of the random matrices, with brief mention of other types of systems (e.g., predator-prey, mutualist, competitor) presented in supporting information. The obvious increase in stability with different variation of $\gamma_{i}$ will be presented, along with an intuitive explanation of why this is the case (still waiting on simulations -- it would be great if correlation between positive or negative processes and response times led to increased stability, as appears to be the case. I could then argue that $Var(\gamma_{i})$ has a minimum such correlation of zero, and can only increase in stabilisation by having an absolute value of correlation being greater than zero). At least two [key figures](#fig_ideas) will be included, likely with four panels (1. without variation, 2. with unstructured variation, and 3. and 4. with structured variation).

7. Some sort of mention is needed for a couple odd results; first, that in mutualist networks, stability is not affected -- perhaps because of the lack of variance in component response rate given number of interaction types (note, I think, competitive networks can have indirect positives due to intransitive effects). Second, *feasibility* is not increased, so pure ecological networks might not be affected by this (but emphasise that *pure* ecological networks don't really exist in nature -- they are incorporated into evolutionary, economic, social, systems).

8. Some bold statement is needed here -- something to the effect of the argument that nearly all large stable complex systems will be stable *due to* the variation simulated in this paper. The odds that it would be stable without such variation becomes vanishingly small as system size increases.

--->

<!---

@Koch1999 Suggests that the results of @May1972 can apply to neuroscience

@May2008 Suggests that random matrix theory can apply to banking

Download: Jansen, V. A., & Kokkoris, G. D. (2003). Complexity and stability revisited. Ecology Letters, 6(6), 498-502.

Hastings, H. M. (1982). The May-Wigner stability theorem. Journal of Theoretical Biology, 97(2), 155-166.

Maybe see how variance in gamma modifies the general findings of May with respect to connectivity and interaction strength? -- Link to variance in off diagonals, which Tang and Allesina (frontiers 2014) finds to be important.

Gray, R. T., & Robinson, P. A. (2008). Stability and synchronization of random brain networks with a distribution of connection strengths. Neurocomputing, 71(7-9), 1373-1387.

Gray & Robinson 2009 Stability of random brain networks with excitatory and inhibitory connections. Neurocomputing

Need to consider that the off-diagonal elements size relative to the diagonal size will affect stability -- higher relative values make things *less* stable. I think this is okay because we're multiplying diagonals and off diagonals by the same values -- this is why if we multiply the whole matrix by a scalar it only affects the magnitude and not the sign of eigenvalues. 

Note further, that Allesina report on the criteria of May that \sigma \sqrt{SC} < 1, meaning that \sigma needs to be low for the random matrix to be stable. Hence, the multiplying rows by $\gamma$ is increasing the stability overall by decreasing the variance? This doesn't seem to make sense though -- really it's adding an element of variation, Var(inherent) + Var(due to \gamma).  But, also maybe should think about gamma in relation to element values. If, e.g., components with similar gammas tend to be more likely to interact, then this might be stabilising? **VARIANCE APPEARS HIGHER GIVEN VARIATION IN gamma**

**Potential solution** -- The increase in variation caused by gamma creates a situation of sometimes increasing but sometimes decreasing the correlation between pairs of off diagonal elements, and sometimes increasing but sometimes decreasing total off-diagonal variation. This increase creates more extreme total variations, some of which cause leading eigenvalues to all be negative. Note that the probability of stability versus instability would be affected by creating disproportionately more extreme variation (high or low) communities that are sometimes stable? **THIS DOES NOT APPEAR TO BE THE CASE BASED ON SIMULATION**
--->




********************************************************************************

**References**

<div id="refs"></div>

********************************************************************************

```{r, echo = FALSE}
A0 <- read.csv(file = "sim_results/bi_gamma/S200_A0.csv");
A0 <- as.matrix(A0[,-1]);
A1 <- read.csv(file = "sim_results/bi_gamma/S200_A1.csv");
A1 <- as.matrix(A1[,-1]);

A0_e   <- eigen(A0)$values;
A0_r   <- Re(A0_e);
A0_i   <- Im(A0_e);
A1_e   <- eigen(A1)$values;
A1_r   <- Re(A1_e);
A1_i   <- Im(A1_e);

A0_vm       <- A0;
diag(A0_vm) <- NA;
A0vec       <- as.vector(t(A0_vm));
A0vec       <- A0vec[is.na(A0vec) == FALSE];
A1_vm       <- A1;
diag(A1_vm) <- NA;
A1vec       <- as.vector(t(A1_vm));
A1vec       <- A1vec[is.na(A1vec) == FALSE];
fhalf       <- 1:(0.5*length(A1vec));
shalf       <- (0.5*length(A1vec)+1):length(A1vec);
pr_st       <- read.csv(file = "sim_results/bi_gamma/bi_pr_st.csv");
pr_st       <- pr_st[,-1];
```

**Figure 1: Example distribution of eigenvalues before (a) and after (b) separating a randomly generated complex system into fast ($\boldsymbol{\gamma} = 1.95$) and slow ($\boldsymbol{\gamma} = 0.05$) component response rates.** Each panel shows the same system where $S = 200$, $C = 0.05$, and $\sigma = 0.4$, and in each case $E[\gamma] = 1$ (i.e., only the distribution of $\gamma$ differs between panels).  **a.** Eigenvalues plotted when all $\gamma = 1$; distributions of points are uniformly distributed within the grey circle with a radius of $\sigma\sqrt{SC} =$ `r round(sqrt(200) * sd(A0vec), digits =3)` centred at -1 on the real axis. **b.** Eigenvalues plotted when half $\gamma = 1.95$ and half $\gamma = 0.05$; distributions of points can be partitioned into one large circle of $\sigma\sqrt{SC} =$ `r round(sqrt(100) * sd(A1vec[fhalf]), digits = 3)` centred at $\gamma = -1.95$ and one small circle of $\sigma\sqrt{SC} =$ `r round(sqrt(100) * sd(A1vec[shalf]), digits = 3)` centred at $\gamma = -0.05$. In a, the maximum real eigenvalaue $\max\left(\Re(\lambda)\right) =$ `r format(max(A0_r), scientific = FALSE)`, while in b $\max\left(\Re(\lambda)\right) =$ `r format(max(A1_r), scientific = FALSE)`, meaning that the complex system in b but not a is stable because in b $\max\left(\Re(\lambda)\right) < 0$. In 1 million randomly generated complex systems under the same parameter values, `r sum(pr_st[,1])` were stable when $\gamma = 1$ while `r sum(pr_st[,2])` were stable when $\gamma = \{1.95, 0.05\}$. Overall, complex systems that are separated into fast versus slow components tend to be more stable than otherwise identical systems with identical component response rates.

```{r, eval = TRUE, echo = FALSE, fig.height = 6, fig.width = 9}
par(mfrow = c(1, 2), mar = c(0.5, 0.5, 0.5, 0.5), oma = c(5, 5, 0, 0));
plot(A0_r, A0_i, xlim = c(-3.7, 0.3), ylim = c(-2, 2), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1);
vl <- seq(from = 0, to = 2*pi, by = 0.001);
A0x0 <- sqrt(200) * sd(A0vec) * cos(vl) + mean(diag(A0));
A0y0 <- sqrt(200) * sd(A0vec) * sin(vl);
text(x = -3.5, y = 2.25, labels = "a", cex = 2);
points(x = A0x0, y = A0y0, type = "l", lwd = 3, col = "grey");
points(A0_r, A0_i, pch = 4, cex = 0.7);

plot(A1_r, A1_i, xlim = c(-3.7, 0.3), ylim = c(-2, 2), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, 
     col = "black", yaxt = "n");

vl <- seq(from = 0, to = 2*pi, by = 0.001);
A0x1a <- sqrt(100) * sd(A1vec[fhalf]) * cos(vl) + mean(diag(A1)[1:100]);
A0y1a <- sqrt(100) * sd(A1vec[fhalf]) * sin(vl);
points(x = A0x1a, y = A0y1a, type = "l", lwd = 3, col = "grey");
A0x1b <- sqrt(100) * sd(A1vec[shalf]) * cos(vl) + mean(diag(A1)[101:200]);
A0y1b <- sqrt(100) * sd(A1vec[shalf]) * sin(vl);
points(x = A0x1b, y = A0y1b, type = "l", lwd = 3, col = "grey");

points(A1_r[1:100], A1_i[1:100],pch = 4, cex = 0.7);   

text(x = -3.5, y = 2.25, labels = "b", cex = 2);
mtext(side = 1, "Real", outer = TRUE, line = 3, cex = 2);
mtext(side = 2, "Imaginary", outer = TRUE, line = 2.5, cex = 2);
```

********************************************************************************

```{r, echo = FALSE}
A_comp <- NULL;
A_dat  <- rnorm(n = 1000000, mean = 0, sd = 0.4);
A_mat  <- matrix(data = A_dat, nrow = 1000);
C_dat  <- rbinom(n = 1000 * 1000, size = 1, prob = 1);
C_mat  <- matrix(data = C_dat, nrow = 1000, ncol = 1000);
A_mat     <- A_mat * C_mat;
gammas <- runif(n = 1000, min = 0, max = 2);
mu_gam <- mean(gammas);
diag(A_mat) <- -1;
A1     <- gammas * A_mat;
A0     <- mu_gam * A_mat;
A0_e   <- eigen(A0)$values;
A0_r   <- Re(A0_e);
A0_i   <- Im(A0_e);
A1_e   <- eigen(A1)$values;
A1_r   <- Re(A1_e);
A1_i   <- Im(A1_e);

A0_vm       <- A0;
diag(A0_vm) <- NA;
A0vec       <- as.vector(A0_vm);
A0vec       <- A0vec[is.na(A0vec) == FALSE];
A1_vm       <- A1;
diag(A1_vm) <- NA;
A1vec       <- as.vector(A1_vm);
A1vec       <- A1vec[is.na(A1vec) == FALSE];
```

**Figure 2: Distributions of eigenvalues before (a) and after (b) introducing variation in component response rate ($\boldsymbol{\gamma}$) in complex systems.** Each panel show the same system where $S = 1000$, $C = 1$, and $\sigma = 0.4$. **a.** Eigenvalues plotted in the absence of $Var(\gamma)$ where $E[\gamma] = 1$, versus **b.** eigenvalues plotted given $\gamma \sim \mathcal{U}(0, 2)$, which increases the variance of interaction strengths ($\sigma^{2}$) but clusters eigenvalues toward the distribution's centre (-1, 0). Black and red elipses in both panels show the circle centred on the distribution in panels a and b, respectively, which have a radius of $\sigma \sqrt{SC}$. Proportions of $\Re(\lambda) < 0$ are `r sum(A0_r < 0) / length(A0_r)` and `r sum(A1_r < 0) / length(A1_r)` for a and b, respectively.

```{r, eval = TRUE, echo = FALSE, fig.height = 6, fig.width = 9}
par(mfrow = c(1, 2), mar = c(0.5, 0.5, 0.5, 0.5), oma = c(5, 5, 0, 0));
plot(A0_r, A0_i, xlim = c(-16.5, 15.5), ylim = c(-16.5,15.5), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1);
vl <- seq(from = 0, to = 2*pi, by = 0.001);
x0 <- sqrt(1000) * sd(A0vec) * cos(vl) + mean(diag(A0));
y0 <- sqrt(1000) * sd(A0vec) * sin(vl);
x1 <- sqrt(1000) * sd(A1vec) * cos(vl) + mean(diag(A1));
y1 <- sqrt(1000) * sd(A1vec) * sin(vl);
text(x = -15.5, y = 19, labels = "a", cex = 2);
points(x = x0, y = y0, type = "l", lwd = 3);
points(x = x1, y = y1, type = "l", col = "red", lwd = 3, lty = "dashed");
plot(A1_r, A1_i, xlim = c(-16.5, 15.5), ylim = c(-16.5,15.5), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, col = "red",
     yaxt = "n");
text(x = -15.5, y = 19, labels = "b", cex = 2);
points(x = x1, y = y1, type = "l", col = "red", lwd = 3)
points(x = x0, y = y0, type = "l", lwd = 3, lty = "dashed");
mtext(side = 1, "Real", outer = TRUE, line = 3, cex = 2);
mtext(side = 2, "Imaginary", outer = TRUE, line = 2.5, cex = 2);
```

********************************************************************************

**Figure 3: Stability of large complex systems with and without variation in component response rate ($\boldsymbol{\gamma}$).** The $\ln$ number of systems that are stable across different system sizes ($S$) given $C = 1$, and the proportion of systems in which variation in $\gamma$ is critical for system stability. For each $S$, 1 million complex systems are randomly generated. Stability of each complex system is tested given variation in $\gamma$ by randomly sampling $\gamma \sim \mathcal{U}(0, 2)$. Stability given $Var(\gamma)$ is then compared to stability in an otherwise identical system in which $\gamma = E[\mathcal{U}(0, 2)]$ for all components. Light and dark grey bars show the number of stable systems in the absence and presence of variance in $\gamma$, respectively. The black line shows the proportion of systems that are stable when $Var(\gamma) > 0$, but would be unstable if $Var(\gamma) = 0$.

```{r, eval = TRUE, echo = FALSE, fig.width = 9, fig.height = 7}
dat <- read.csv(file = "sim_results/C_1/random_all.csv");
dat <- dat[,-1];
plot_stables(dat);
```

********************************************************************************

**Figure 4: Distributions of variation in component response time evolved to generate stable systems**



















