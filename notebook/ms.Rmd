---
title: "Component response rate variation underlies the stability of complex systems"
author: "A. Bradley Duthie    ( alexander.duthie@stir.ac.uk )"
date: Biological and Environmental Sciences, University of Stirling, Stirling, UK,
  FK9 4LA
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
  html_document: default
  word_document:
    fig_caption: yes
    pandoc_args:
    - --csl
    - nature.csl
    reference_docx: docx_template.docx
bibliography: references.bib
header-includes:
- \usepackage{amsmath}
- \usepackage{natbib}
- \usepackage{lineno}
- \usepackage[utf8]{inputenc}
- \linenumbers
- \bibliographystyle{amnatnat}
linestretch: 2
link-citations: yes
linkcolor: blue
csl: nature.csl
biblio-style: apalike
---

<!---
Abstract current word count: 225
Text current word count: 1453
--->

<!---
"This will generate the intermediate files that are actually used to generate the PDF. You need only go to the directory where your .Rmd file is, gather the .tex and image directory (called filename_files) with"

arxiv comand: tar cvfz filename.tgz filename.tex filename_files

https://gist.github.com/JJ/ef9d3d8f1142df064bd5
--->

<!---

NOTE: Consider looking at the resilience of the stable networks given $\gamma = 1$ and $Var(\gamma)$, which is defined by the inverse of the leading eigenvalue. The inverse of the real part of the leading eigenvalue indicates how quickly the system returns to equilibrium (resilience). See @Landi2018 for more. Intuitively, resilience should perhaps increase a bit if we're only looking at the subset of systems that are stable (many of which will have had their eigenvalues shifted left)?

Landi, P., Minoarivelo, H. O., Brännström, Å., Hui, C., & Dieckmann, U. (2018). Complexity and stability of ecological networks: A review of the theory. Population Ecology, 0(0), In press. https://doi.org/10.1007/s10144-018-0628-3

--->

<!---

1. As above, check out the change in resilience defined as the difference beween the inverse of the real part of the leading eigenvalue in $Var(\gamma)$ versus $\gamma = 1$. Initial simulaitons suggest that resilience is increased on average by $Var(\gamma)$, and increasingly so as S increases.

2. Check for induced correlations causing stabilisation (and E1, E2, and EC, more generally), as per Reviewer 2 specific comment 3. If possible, give an analytical explanation for the increase in stability that has to do with the error inducing changes in correlations.

--->



```{r, echo = FALSE}
source(file = "R_old/sim_mat.R");
source(file = "R_old/plot_figs.R");
```

```{r, echo = FALSE}
# Temporary place for revised functions
plot_Fig_1 <- function(A0, A1){
    S_val       <- dim(A0)[1];
    A0_e        <- eigen(A0)$values;
    A0_r        <- Re(A0_e);
    A0_i        <- Im(A0_e);
    A1_e        <- eigen(A1)$values;
    A1_r        <- Re(A1_e);
    A1_i        <- Im(A1_e);
    A0_vm       <- A0;
    diag(A0_vm) <- NA;
    A0vec       <- as.vector(t(A0_vm));
    A0vec       <- A0vec[is.na(A0vec) == FALSE];
    A1_vm       <- A1;
    diag(A1_vm) <- NA;
    A1vec       <- as.vector(t(A1_vm));
    A1vec       <- A1vec[is.na(A1vec) == FALSE];
    fhalf       <- 1:(0.5*length(A1vec));
    shalf       <- (0.5*length(A1vec)+1):length(A1vec);
    par(mfrow = c(1, 2), mar = c(0.5, 0.5, 0.5, 0.5), oma = c(5, 5, 0, 0));
    plot(A0_r, A0_i, xlim = c(-3.7, 0.3), ylim = c(-2, 2), pch = 4, cex = 0.7,
         xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1);
    vl   <- seq(from = 0, to = 2*pi, by = 0.001);
    A0x0 <- sqrt(S_val) * sd(A0vec) * cos(vl) + mean(diag(A0));
    A0y0 <- sqrt(S_val) * sd(A0vec) * sin(vl);
    text(x = -3.5, y = 2.25, labels = "a", cex = 2);
    points(x = A0x0, y = A0y0, type = "l", lwd = 3, col = "dodgerblue4");
    points(A0_r, A0_i, pch = 4, cex = 0.7);
    plot(A1_r, A1_i, xlim = c(-3.7, 0.3), ylim = c(-2, 2), pch = 4, cex = 0.7,
         xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, 
         col = "dodgerblue4", yaxt = "n");
    vl <- seq(from = 0, to = 2*pi, by = 0.001);
    A0x1a <- sqrt(0.5*S_val) * sd(A1vec[fhalf]) * cos(vl) + 
                 mean(diag(A1)[1:(0.5*S_val)]);
    A0y1a <- sqrt(S_val) * sd(A1vec[fhalf]) * sin(vl);
    #points(x = A0x1a, y = A0y1a, type = "l", lwd = 3, col = "grey");
    A0x1b <- sqrt(0.5*S_val) * sd(A1vec[shalf]) * cos(vl) + 
        mean( diag(A1)[( (0.5*S_val) + 1 ):S_val] );
    A0y1b <- sqrt(0.5*S_val) * sd(A1vec[shalf]) * sin(vl);
    #points(x = A0x1b, y = A0y1b, type = "l", lwd = 3, col = "grey");
    points(A1_r[1:S_val], A1_i[1:S_val],pch = 4, cex = 0.7, col = "firebrick");   
    text(x = -3.5, y = 2.25, labels = "b", cex = 2);
    mtext(side = 1, "Real", outer = TRUE, line = 3, cex = 2);
    mtext(side = 2, "Imaginary", outer = TRUE, line = 2.5, cex = 2);
}

plot_Fig_2 <- function(){
    A_comp <- NULL;
    A_dat  <- rnorm(n = 1000000, mean = 0, sd = 0.4);
    A_mat  <- matrix(data = A_dat, nrow = 1000);
    C_dat  <- rbinom(n = 1000 * 1000, size = 1, prob = 1);
    C_mat  <- matrix(data = C_dat, nrow = 1000, ncol = 1000);
    A_mat     <- A_mat * C_mat;
    gammas <- runif(n = 1000, min = 0, max = 2);
    mu_gam <- mean(gammas);
    diag(A_mat) <- -1;
    A1     <- gammas * A_mat;
    A0     <- mu_gam * A_mat;
    A0_e   <- eigen(A0)$values;
    A0_r   <- Re(A0_e);
    A0_i   <- Im(A0_e);
    A1_e   <- eigen(A1)$values;
    A1_r   <- Re(A1_e);
    A1_i   <- Im(A1_e);
    A0_vm       <- A0;
    diag(A0_vm) <- NA;
    A0vec       <- as.vector(A0_vm);
    A0vec       <- A0vec[is.na(A0vec) == FALSE];
    A1_vm       <- A1;
    diag(A1_vm) <- NA;
    A1vec       <- as.vector(A1_vm);
    A1vec       <- A1vec[is.na(A1vec) == FALSE];
    par(mfrow = c(1, 2), mar = c(0.5, 0.5, 0.5, 0.5), oma = c(5, 5, 0, 0));
    plot(A0_r, A0_i, xlim = c(-16.5, 15.5), ylim = c(-16.5,15.5), pch = 4, 
         cex = 0.7, xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, 
         asp = 1, col = "dodgerblue4");
    vl <- seq(from = 0, to = 2*pi, by = 0.001);
    x0 <- sqrt(1000) * sd(A0vec) * cos(vl) + mean(diag(A0));
    y0 <- sqrt(1000) * sd(A0vec) * sin(vl);
    x1 <- sqrt(1000) * sd(A1vec) * cos(vl) + mean(diag(A1));
    y1 <- sqrt(1000) * sd(A1vec) * sin(vl);
    text(x = -15.5, y = 19, labels = "a", cex = 2);
    points(x = x0, y = y0, type = "l", lwd = 3, col = "dodgerblue4");
    points(x = x1, y = y1, type = "l", col = "red", lwd = 3, lty = "dashed");
    plot(A1_r, A1_i, xlim = c(-16.5, 15.5), ylim = c(-16.5,15.5), pch = 4, 
         cex = 0.7, xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, 
         asp = 1, col = "firebrick", yaxt = "n");
    text(x = -15.5, y = 19, labels = "b", cex = 2);
    points(x = x1, y = y1, type = "l", col = "firebrick", lwd = 3);
    points(x = x0, y = y0, type = "l", lwd = 3, lty = "dashed");
    mtext(side = 1, "Real", outer = TRUE, line = 3, cex = 2);
    mtext(side = 2, "Imaginary", outer = TRUE, line = 2.5, cex = 2);
}

plot_stables <- function(dat, S_s = 32){
    Ns          <- 1:S_s;
    par(oma = c(6, 6, 1, 6), mar = c(0.5, 0.5, 0.5, 0.5));
    #=================================
    bar_dat                      <- t(cbind(dat[Ns,3], dat[Ns,5]));
    log_bar_dat                  <- log(bar_dat);
    log_bar_dat[log_bar_dat < 0] <- 0; 
    barplot(log_bar_dat, beside = TRUE, col = c("dodgerblue4", "firebrick"),
            names.arg = dat[Ns,1], ylim = c(0, 16), xlab = "",
            ylab = "Ln number of stable communities", cex.lab = 1, 
            cex.axis = 1.25, xlim = c(1, 94), cex.names = 1, yaxt = "n");
    axis(side = 2, at = c(0, 2, 4, 6, 8, 10, 12, 14), cex.axis = 1.5);
    box(lwd = 2);
    par(new = TRUE);
    y1     <- dat[1:S_s,6] / (dat[1:S_s,5]);
    x1     <- seq(from = 2.132, to = 15.1112, length = S_s);
    plot(x = x1, y = y1, xaxt = "n", yaxt = "n", lwd = 2, ylim = c(0, 1.1),
         xlab = "", ylab = "", type = "b", xlim = c(2, 15), pch = 20, 
         cex = 1, col = "black", yaxs="i");
    points(x = x1, y = y1, lwd = 2, type = "l", col = "black");
    axis(side = 4, at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0), cex.axis = 1.5);
    legend("topleft", c(expression(paste(gamma," = 1")), 
                        expression(paste("Var(",gamma,")"))), 
           pch=15, col=c("dodgerblue4","firebrick"), cex = 1.5, horiz = TRUE);
    #=================================
    mtext(side = 1, text = "System size (S)", cex = 2, outer = TRUE, 
          line = 3.0);
    mtext(side = 2, text = "Ln number of stable systems", cex = 2, 
          outer = TRUE, line = 3.5);
    mtext(side = 4, text = expression(
        paste("Pr. of systems stable due to Var(",gamma,")")),
        cex = 2, outer = TRUE, line = 3.5);
}

plot_stable_4 <- function(dat, S_s = 39){
    Ns          <- 1:S_s;
    par(oma = c(6, 6, 1, 6), mar = c(0.5, 0.5, 0.5, 0.5));
    #=================================
    bar_dat                      <- t(cbind(dat[Ns,3], dat[Ns,5]));
    log_bar_dat                  <- log(bar_dat);
    log_bar_dat[log_bar_dat < 0] <- 0; 
    barplot(log_bar_dat, beside = TRUE, col = c("dodgerblue4", "firebrick"),
            names.arg = dat[Ns,1], ylim = c(0, 14), xlab = "",
            ylab = "Ln number of stable communities", cex.lab = 1, 
            cex.axis = 1.25, xlim = c(1, 116), cex.names = 1, yaxt = "n");
    axis(side = 2, at = c(0, 2, 4, 6, 8, 10, 12), cex.axis = 1.5);
    box(lwd = 2);
    par(new = TRUE);
    y1     <- dat[1:S_s,6] / (dat[1:S_s,5]);
    x1     <- seq(from = 2.132, to = 15.1112, length = S_s);
    plot(x = x1, y = y1, xaxt = "n", yaxt = "n", lwd = 2, ylim = c(0, 1.1),
         xlab = "", ylab = "", type = "b", xlim = c(2, 15), pch = 20, 
         cex = 1, col = "black", yaxs="i");
    points(x = x1, y = y1, lwd = 2, type = "l", col = "black");
    axis(side = 4, at = c(0, 0.2, 0.4, 0.6, 0.8, 1.0), cex.axis = 1.5);
    legend("topleft", c(expression(paste(gamma," = 1")), 
                        expression(paste("Var(",gamma,")"))), 
           pch=15, col=c("dodgerblue4","firebrick"), cex = 1.5, horiz = TRUE);
    #=================================
    mtext(side = 1, text = "System size (S)", cex = 2, outer = TRUE, 
          line = 3.0);
    mtext(side = 2, text = "Ln number of stable systems", cex = 2, 
          outer = TRUE, line = 3.5);
    mtext(side = 4, text = expression(
        paste("Pr. of systems stable due to Var(",gamma,")")),
        cex = 2, outer = TRUE, line = 3.5);
}
```



**Key words:** Ecological networks, gene-regulatory networks, neural networks, financial networks, system stability, random matrix theory


Abstract
--------------------------------------------------------------------------------

The stability of a complex system generally decreases with increasing system size and interconnectivity, a counterintuitive result of widespread importance across the physical, life, and social sciences. Despite recent interest in the relationship between system properties and stability, the effect of variation in the response rate of individual system components remains unconsidered. Here I vary the component response rates ($\boldsymbol{\gamma}$) of randomly generated complex systems. I use numerical simulations to show that when component response rates vary, the potential for system stability is markedly increased. These results are robust to common network structures, including small-world and scale-free networks, and cascade food webs. Variation in $\boldsymbol{\gamma}$ is especially important for stability in highly complex systems, in which the probability of stability would otherwise be negligible. At such extremes of simulated system complexity, the largest stable complex systems would be unstable if not for $\boldsymbol{Var(\gamma)}$. My results therefore reveal a previously unconsidered aspect of system stability that is likely to be pervasive across all realistic complex systems.


 

<!---

Author Summary
--------------------------------------------------------------------------------

Any set of discrete components that potentially interact can define a complex system. Typically, properties of individual components will cause them to respond to system perturbation at different rates, but the consequences of such variation for system stability are unexplored. This work makes three novel theoretical contributions: First, I show that large complex systems with varying component response rates are more likely to be stable than when response rates are uniform. Second, I show that a targeted manipulation of component response rates can increase the probability of system stability by several orders of magnitude. Lastly, I show that varying component response rate does not affect system feasibility. These results are general to any physical, biological, social, or mixed system.

--->

Introduction
--------------------------------------------------------------------------------

In 1972, May [@May1972] first demonstrated that randomly assembled systems of sufficient complexity are almost inevitably unstable given infinitesimally small perturbations. Complexity in this case is defined by the size of the system (i.e., the number of potentially interacting components; $S$), its connectance (i.e., the probability that one component will interact with another; $C$), and the variance of interaction strengths ($\sigma^{2}$) [@Allesina2012]. May's finding that the probability of local stability falls to near zero given a sufficiently high threshold of $\sigma\sqrt{SC}$ is broadly relevant for understanding the dynamics and persistence of systems such as ecological [@May1972; @Allesina2012; @Mougi2012; @Allesina2015; @Grilli2017], neurological [@Gray2008; @Gray2009], biochemical [@Rosenfeld2009; @MacArthur2010], and socio-economic [e.g., banking; @May2008; @Haldane2011; @Suweis2014; @Bardoscia2017] networks. As such, identifying general principles that affect stability in complex systems is of wide-ranging importance.

Randomly assembled complex systems can be represented as large square matrices ($\mathbf{M}$) with $S$ components (e.g., networks of species [@Allesina2012] or banks [@Haldane2011]). One element of such a matrix, $M_{ij}$, defines how component $j$ affects component $i$ in the system at a point of equilibrium [@Allesina2012]. Off-diagonal elements ($i \neq j$) therefore define interactions between components, while diagonal elements ($i = j$) define component self-regulation (e.g., carrying capacity in ecological communities). Traditionally, off-diagonal elements are assigned non-zero values with a probability $C$, which are sampled from a distribution with variance $\sigma^{2}$; diagonal elements are set to -1 [@May1972; @Allesina2012; @Allesina2015]. Local system stability is assessed using eigenanalysis, with the system being stable if the real parts of all eigenvalues ($\lambda$) of $\mathbf{M}$ are negative ($\max\left(\Re(\lambda)\right) < 0$) [@May1972; @Allesina2012]. In a large system (high $S$), eigenvalues are distributed uniformly [@Tao2010] within a circle centred at $\Re = -1$ (the mean value of diagonal elements) and $\Im = 0$, with a radius of $\sigma\sqrt{SC}$ [@May1972; @Allesina2012; @Allesina2015] (Figs 1a and 2a). Local stability of randomly assembled systems therefore becomes increasingly unlikely as $S$, $C$, and $\sigma^{2}$ increase.

May's [@May1972; @Allesina2012] stability criterion $\sigma\sqrt{SC} < 1$ assumes that the expected response rates ($\gamma$) of individual components to perturbations of the system are identical, but this is highly unlikely in any complex system. In ecological communities, for example, the rate at which population density changes following perturbation will depend on the generation time of organisms, which might vary by orders of magnitude among species. Species with short generation times will respond quickly (high $\gamma$) to perturbations relative to species with long generation times (low $\gamma$). Similarly, the speed at which individual banks respond to perturbations in financial networks, or individuals or institutions respond to perturbations in complex social networks, is likely to vary. The effect of such variance on stability has not been investigated in complex systems theory. Intuitively, variation in $\gamma$ might be expected to decrease system stability by introducing a new source of variation into the system and thereby increasing $\sigma$. Here I show why, despite higher $\sigma$, realistic complex systems (such that $S$ is high but finite) are actually more likely to be stable if their individual component response rates vary. <!--- Something about including realistic structured networks (scale free, small world, and cascade food webs) --->

```{r echo = FALSE}
S     <- 200;
C     <- 0.05;
sigma <- 0.4;
pr_st <- read.csv(file = "sim_results/bi_gamma/bi_pr_st.csv");
pr_st <- pr_st[,-1];
```


```{r, echo = FALSE}
A0 <- read.csv(file = "sim_results/bi_gamma/S200_A0.csv");
A0 <- as.matrix(A0[,-1]);
A1 <- read.csv(file = "sim_results/bi_gamma/S200_A1.csv");
A1 <- as.matrix(A1[,-1]);

A0_e   <- eigen(A0)$values;
A0_r   <- Re(A0_e);
A0_i   <- Im(A0_e);
A1_e   <- eigen(A1)$values;
A1_r   <- Re(A1_e);
A1_i   <- Im(A1_e);

A0_vm       <- A0;
diag(A0_vm) <- NA;
A0vec       <- as.vector(t(A0_vm));
A0vec       <- A0vec[is.na(A0vec) == FALSE];
A1_vm       <- A1;
diag(A1_vm) <- NA;
A1vec       <- as.vector(t(A1_vm));
A1vec       <- A1vec[is.na(A1vec) == FALSE];
fhalf       <- 1:(0.5*length(A1vec));
shalf       <- (0.5*length(A1vec)+1):length(A1vec);
pr_st       <- read.csv(file = "sim_results/bi_gamma/bi_pr_st.csv");
pr_st       <- pr_st[,-1];
```

Results
--------------------------------------------------------------------------------

**Component response rates of random complex systems**. Rows in $\mathbf{M}$ define how a given component $i$ is affected by other components of the system, meaning that the rate of component response time can be modelled by multiplying all row elements by a real scalar value $\gamma_{i}$ [@Patel2018]. The distribution of $\gamma$ over $S$ components thereby models the distribution of component response rates.  <!--- Replace the below with some more informative text concerning the variation that $\var(gamma})$ adds to the system $M$ overall. This might require starting with just $M = A$ as an explanation. Borrowing the text from revision_notes.Rmd below --->

<!--- The current Figure 1 should be replaced by the new figure first shown in revision_notes.Rmd --->


<!---

If the off-diagonal elements of $\textbf{A}$ are sampled independently from an identical distribution, then  $\max(\Re(\lambda))$ for $\textbf{M} = \textbf{A}$ can be estimated from five values [@Tang2014b]. These values include (1) system size ($S$), (2) mean self-regulation of components ($d$), (3) mean interaction strength between components ($\mu$), (4) the standard deviation between component interaction strengths ($\sigma$), and (5) the correlation of interaction strengths between components, $M_{ij}$ and $M_{ji}$ ($\rho$). When investigating the effect of varying component response rate $Var(\gamma)$ on stability by defining $\textbf{M} = \gamma\textbf{A}$, $S$ remains unchanged. Further, values of $\gamma_{i}$ were sampled such that $E[d]$ and $E[\mu]$ also remained unchanged (in practice, diagonal elements of $\textbf{M}$ were standardised so that mean values were identical before and after adding $\gamma$). What $Var(\gamma)$ does change is the variation in component interaction strengths, and $\rho$.

Introducing variation in $\gamma$ increases the total variation in the system, making it more likely that $\textbf{M} = \gamma\textbf{A}$ is unstable. Variation of the off-diagonal elements in $\textbf{M}$ is described by the joint variation of two random variables (to simplify the notation, $\sigma^{2}_{M}$ and $\sigma^{2}_{A}$ refer to variances of off-diagonal elements only),

$$\sigma^{2}_{M} = \sigma^{2}_{A}\sigma^{2}_{\gamma} + \sigma^{2}_{A}E[\gamma_{i}]^{2}+\sigma^{2}_{\gamma}E[A_{ij}]^{2}.$$

Note that in my simulations $E[\gamma_{i}] = 1$ and $E[A_{ij}] = 0$, so the above can be simplified,

$$\sigma^{2}_{M} = \sigma^{2}_{A}(1 + \sigma^{2}_{\gamma}).$$

The increase caused by $\sigma^{2}_\gamma$ can be visualised from the eigenvalue spectra of $\textbf{A}$ versus $\textbf{M} = \gamma\textbf{A}$. Given $d = 0$ and $C = 1$, the distribution of eigenvalues of $\textbf{M}$ and $\textbf{A}$ lie within a circle of a radius $\sigma^{2}_{M}\sqrt{S}$ and $\sigma^{2}_{A}\sqrt{S}$, respectively.

--->


<!--- Remove this instructive example; replace it with something better. This entire paragraph and the code underneath it can be deleted. --->
An instructive example compares one $\mathbf{M}$ where $\gamma_{i} = 1$ for all $i$ in $S$ to the same $\mathbf{M}$ when half of $\gamma_{i} = 1.95$ and half of $\gamma_{i} = 0.05$. This models one system in which $\gamma$ is invariant and one in which $\gamma$ varies, but systems are otherwise identical (note that mean $\gamma_{i} = 1$ in both cases). I assume $S = 200$, $C = 0.05$, and $\sigma = 0.4$; diagonal elements are set to $-1$ and non-zero off-diagonal elements are drawn randomly from $\mathcal{N}(0, \sigma^{2})$. Rows are then multiplied by $\gamma_{i}$ to generate $\mathbf{M}$. When $\gamma_{i} = 1$, eigenvalues of $\mathbf{M}$ are distributed uniformly within a circle centred at ($-1, 0$) with a radius of `r round(sigma*sqrt(S*C), digits = 3)` (Fig. 1a). Hence, the real components of eigenvalues are highly unlikely to all be negative when all $\gamma_{i} = 1$. But when $\gamma_{i}$ values are separated into two groups, eigenvalues are no longer uniformly distributed (Fig. 1b). Instead, two distinct clusters of eigenvalues appear (red circles in Fig. 1b), one centred at ($-1.95, 0$) and the other centred at ($-0.05, 0$). The former has a large radius, but the real components have shifted to the left (in comparison to when $\gamma = 1$), while the latter cluster has a smaller radius, but real components have shifted to the right; all $\Re({\lambda}) < 0$. Overall, for 1 million randomly assembled $\mathbf{M}$, this division between slow and fast component response rates results in more stable systems: `r sum(pr_st[,1])` stable given $\gamma = 1$ versus `r sum(pr_st[,2])` stable given $\gamma = \{1.95, 0.5\}$.

```{r echo = FALSE}
S     <- 1000;
C     <- 0.05;
sigma <- 0.4;
```


```{r, echo = FALSE}
A_comp <- NULL;
A_dat  <- rnorm(n = 1000000, mean = 0, sd = 0.4);
A_mat  <- matrix(data = A_dat, nrow = 1000);
C_dat  <- rbinom(n = 1000 * 1000, size = 1, prob = 0.05);
C_mat  <- matrix(data = C_dat, nrow = 1000, ncol = 1000);
A_mat     <- A_mat * C_mat;
A_mat  <- species_interactions(A_mat, type = 0);
gammas <- c(rep(0.05, 500), rep(1.95, 500));

mu_gam <- mean(gammas);
diag(A_mat) <- -1;
A1     <- gammas * A_mat;
A0     <- mu_gam * A_mat;
A0_e   <- eigen(A0)$values;
A0_r   <- Re(A0_e);
A0_i   <- Im(A0_e);
A1_e   <- eigen(A1)$values;
A1_r   <- Re(A1_e);
A1_i   <- Im(A1_e);

A0_vm       <- A0;
diag(A0_vm) <- NA;
A0vec       <- as.vector(A0_vm);
A0vec       <- A0vec[is.na(A0vec) == FALSE];
A1_vm       <- A1;
diag(A1_vm) <- NA;
A1vec       <- as.vector(A1_vm[1:500,]);
A1vec       <- A1vec[is.na(A1vec) == FALSE];
```


<!--- Some of this paragraph can be removed. Some of the middle can focus on the new Figure 1 panels C and D, which illustrate the effect of gamma on the eigenvalue spectra. The rest, focusing on the results, can be retained.--->
Higher stability in systems with variation in $\gamma$ can be observed by sampling $\gamma_{i}$ values from various distributions. I focus on a uniform distribution where $\gamma \sim \mathcal{U}(0, 2)$ (see Supplementary Information for other distributions, which give similar results). <!--- Cut from here ---> As with the case of $\gamma = \{1.95, 0.5\}$ (Fig. 1b), mean $\gamma = 1$ when $\gamma \sim \mathcal{U}(0, 2)$, allowing comparison of $\mathbf{M}$ before and after the addition of variation in component response rate. Figure 2 shows a comparison of eigenvalue distributions given $S = 1000$, $C = 0.05$, and $\sigma = 0.4$. <!--- end cut here, but now refer to Figure 1 panels C and D ---> As expected [@Tao2010], when $\gamma = 1$, eigenvalues are distributed uniformly in a circle centred at ($-1, 0$) with a radius of $\sigma\sqrt{SC} =$ `r round(sigma*sqrt(S*C), digits = 3)`. Uniform variation in $\gamma$ leads to a non-uniform distribution of eigenvalues [@Ahmadian2015; @Gibbs2017; @Stone2017], some of which are clustered locally near the centre of the distribution, but others of which are spread outside the former radius of `r round(sigma*sqrt(S*C), digits = 3)` (Fig 2b). The clustering and spreading of eigenvalues introduced by $Var(\gamma)$ can destabilise previously stable systems or stabilise systems that are otherwise unstable. But where systems are otherwise too complex to be stable given $\gamma = 1$, the effect of $Var(\gamma)$ can often lead to stability above May's [@May1972; @Allesina2012] threshold $\sigma\sqrt{SC} < 1$. <!--- Okay, so most can be retained --->

```{r, echo = FALSE}
dat <- read.csv(file = "sim_results/C_1/random_all.csv");
dat <- dat[,-1];
```

**Simulation of random $\mathbf{M}$ across $\mathbf{S}$**. To investigate the effect of $Var(\gamma)$ on stability across systems of varying complexity, I simulated random $\mathbf{M}$ matrices at $\sigma = 0.4$ and $C = 1$ across $S = \{2, 3, ..., 49, 50\}$. One million $\mathbf{M}$ were simulated for each $S$, and the stability of $\mathbf{M}$ was assessed given $\gamma = 1$ versus $\gamma \sim \mathcal{U}(0, 2)$. For all $S > 10$, I found that the number of stable random systems was higher given $Var(\gamma)$ than when $\gamma = 1$ (Fig. 3; see Supplementary Information for full table of results), and that the difference between the probabilities of observing a stable system increased with an increase in $S$. In other words, the potential for $Var(\gamma)$ to affect stability increased with system complexity and was most relevant for systems on the cusp of being too complex to be realistically stable. For the highest values of $S$, nearly all systems that were stable given $Var(\gamma)$ would not have been stable given $\gamma = 1$.

<!--- Here a new section is needed --->
<!--- 

**Effects of $\gamma$ and correlated interactions on stability**

This is going to require that a new figure be inserted showing correlation versus stability. Some general points about how $rho$ is known to affect stability is also needed, but this should not be too difficult to explain in a sentence or two. 

--->

<!--- Another new section is needed --->
<!--- 

**Effects of $\gamma$ on structured networks**

This will report results for small-world, scale-free, and cascade food-webs. It will also briefly mention random predator-prey, mutualist, and competitor networks. These resutls will require a small table, at least.

--->



<!--- Tempting to remove this section entirely, but if there is enough space for it, then maybe retain it; reviewers did not mention a problem with it, and it's a short and relevant point to make.  In any case, it should be swapped with the section on feasibility below. --->
**Targeted manipulation of $\mathbf{\gamma}$**. To further investigate the potential of $Var(\gamma)$ to be stabilising, I used a genetic algorithm. Genetic algorithms are heuristic tools that mimic evolution by natural selection, and are useful when the space of potential solutions (in this case, possible combinations of $\gamma$ values leading to stability in a complex system) is too large to search exhaustively [@Hamblin2013]. Generations of selection on $\gamma$ value combinations to minimise $\max\left(\Re(\lambda)\right)$ demonstrated the potential for $Var(\gamma)$ to increase system stability. Across $S = \{2, 3, ..., 39, 40\}$, sets of $\gamma$ values were found that resulted in stable systems with probabilities that were up to four orders of magnitude higher than when $\gamma = 1$ (Fig. 4), meaning that stability could often be achieved by manipulating $S$ $\gamma$ values rather than $S \times S$ $\mathbf{M}$ elements (i.e., by manipulating component response rates rather than interactions between components). 

<!--- I don't think much here needs to change --->
**System feasibility given $\mathbf{Var(\gamma)}$** For complex systems in which individual system components represent the density of some tangible quantity, it is relevant to consider the feasibility of the system. Feasibilility assumes that values of all components are positive at equilibrium [@Grilli2017; @Dougoud2018; @Song2018]. This is of particular interest for ecological communities because population density ($N$) cannot take negative values, meaning that ecological systems need to be feasible for stability to be biologically realistic [@Dougoud2018]. While my results are intended to be general to all complex systems, and not restricted to species networks, I have also performed a feasibility analysis on all matrices $\mathbf{M}$ tested for stability, and additionally for specific types of ecological communities [@Allesina2012] (e.g., competitive, mutualist, predator-prey; see Supplementary Information). I emphasise that $\gamma$ is not interpreted as population density in this analysis, but instead as a fundamental property of species life history such as expected generation time. Feasibility was unaffected by $Var(\gamma)$ and instead occurred with a fixed probability of $1/2^{S}$, consistent with a recent proof by Serv&aacute;n et al. [@Servan2018] (see Supplementary Information). Hence, for pure interacting species networks, variation in component response rate (i.e., species generation time) does not affect stability at biologically realistic species densities. 


Discussion
--------------------------------------------------------------------------------

I have shown that the stability of complex systems might often be contigent upon variation in the response rates of their individual components, meaning that factors such as rate of trait evolution (in biological networks), transaction speed (in economic networks), or communication speed (in social networks) need to be considered when investigating the stability of complex systems. Variation in component response rate is more likely to be critical for stability in systems that are especially complex, and it can ultimately increase the probability that system stability is observed above that predicted by May's [@May1972] classically derived $\sigma \sqrt{SC}$ criterion. The logic outlined here is general, and potentially applies to any complex system in which individual system components can vary in their reaction rates to system perturbation.

It is important to recognise that variation in component response rate is not stabilising per se; that is, adding variation in component response rates to a particular system does not increase the probability that the system will be stable. Rather, highly complex systems that are observed to be stable are more likely to have varying component response rates, and for this variation to be critical to their stability (Fig. 3). This is caused by the shift to a non-uniform distribution of eigenvalues that occurs by introducing $Var(\gamma)$ (Fig. 1b, 2b), which can sometimes cause all of the real components of the eigenvalues of the system matrix to become negative, but might also increase the real components of eigenvalues. 

My focus is distinct from Gibbs et al. [@Gibbs2017], who applied the same mathematical framework to investigate how a diagonal matrix $\mathbf{X}$ (equivalent to $\gamma$ in my model) affects the stability of a community matrix $\mathbf{M}$ given an interaction matrix $\mathbf{A}$ within a generalised Lotka-Volterra model, where $\mathbf{M} = \mathbf{XA}$. Gibbs et al. [@Gibbs2017] analytically demonstrated that the effect of $\mathbf{X}$ on system stability decreases exponentially as system size becomes arbitrarily large ($S \to \infty$) for a given magnitude of complexity $\sigma\sqrt{SC}$. My numerical results do not contradict this prediction because I did not scale $\sigma = 1 / \sqrt{S}$, but instead fixed $\sigma$ and increased $S$ to thereby increase total system complexity (see Supplemental Information for results simulated across $\sigma$ and $C$). Overall, I show that component response rate variation increases the upper bound of complexity at which stability can be realistically observed, meaning that highly complex systems are more likely than not to vary in their component response rates, and for this variation to be critical for system stability.

The potential importance of component response rate variation was most evident from the results of simulations in which the genetic algorithm was used in attempt to maximise the probability of system stability. The probability that some combination of component response rates could be found to stabilise the system was shown to be up to four orders of magnitude higher than the background probabilities of stability in the absence of any component response rate variation. Instead of manipulating the $S \times S$ interactions between system components, it might therefore be possible to manipulate only the $S$ response rates of individual system components to achieve stability. Hence, managing the response rates of system components in a targeted way could potentially facilitate the stabilisation of complex systems through a reduction in dimensionality. 

Interestingly, while complex systems were more likely to be stable given variation in component response rate, they were not more likely to be feasible, meaning that stability was not increased when component values were also restricted to being positive at equilibrium. Feasibility is important to consider, particularly for the study of ecological networks of species [@Stone2017; @Grilli2017; @Dougoud2018; @Servan2018] because population densities cannot realistically be negative. My results therefore suggest that variation in the rate of population responses to perturbation (e.g., due to differences in generation time among species) is unlikely to be critical to the stability of purely multi-species interaction networks (see also Supplementary Information). Nevertheless, ecological interactions do not exist in isolation in empirical systems [@Patel2018], but instead interact with evolutionary, abiotic, or social-economic systems. The relevance of component response rate for complex system stability should therefore not be ignored in the broader context of ecological communities.

A general mathematical framework encompassing shifts in eigenvalue distributions caused by a vector $\gamma$ has been investigated [@Ahmadian2015] and recently applied to questions concerning species density and feasibility [@Gibbs2017; @Stone2017], but $\gamma$ has not been interpreted as rates of response of individual system components to perturbation. My model focuses on component response rates for systems of a finite size, in which complexity is high but not yet high enough to make the probability of stability unrealistically low for actual empirical systems. For this upper range of system size, randomly assembled complex systems are more likely to be stable if their component response rates vary (e.g., $10 < S < 30$ for parameter values in Fig. 3). Overall, I suggest that variation in component response rate might therefore be critical for maintaining stability in many highly complex empirical systems. These results are broadly applicable for understanding the stability of complex networks across the physical, life, and social sciences.

Methods
--------------------------------------------------------------------------------

**Component response rate variation ($\mathbf{\gamma}$)**. In a synthesis of eco-evolutionary feedbacks on community stability, Patel et al. model a system that includes a vector of potentially changing species densities ($\mathbf{N}$) and a vector of potentially evolving traits ($\mathbf{x}$) [@Patel2018]. For any species $i$ or trait $j$, change in species density ($N_{i}$) or trait value ($x_{j}$) with time ($t$) is a function of the vectors $\mathbf{N}$ and $\mathbf{x}$,

$$\frac{dN_{i}}{dt} = N_{i}f_{i}(\mathbf{N}, \mathbf{x}),$$

$$\frac{dx_{j}}{dt} = \epsilon g_{j}(\mathbf{N}, \mathbf{x}).$$

In the above, $f_{i}$ and $g_{j}$ are functions that define the effects of all species densities and trait values on the density of a species $i$ and the value of trait $j$, respectively. Patel et al. were interested in stability when the evolution of traits was relatively slow or fast in comparison with the change in species densities [@Patel2018], and this is modulated in the above by the scalar $\epsilon$. The value of $\epsilon$ thereby determines the timescale separation between ecology and evolution, with high $\epsilon$ modelling relatively fast evolution and low $\epsilon$ modelling relative slow evolution [@Patel2018]. 

I use the same principle that Patel et al. use to modulate the relative rate of evolution to modulate rates of component responses for $S$ components. Following May [@May1972; @May1973], the value of a component $i$ at time $t$ ($v_{i}(t)$) is affected by the value of $j$ ($v_{j}(t)$) and $j$'s marginal effect on $i$ ($a_{ij}$), and by $i$'s response rate ($\gamma_{i}$),

$$\frac{dv_{i}(t)}{dt} = \gamma_{i} \sum_{j=1}^{S}a_{ij}v_{j}(t).$$

In matrix notation [@May1973],

$$\frac{d\mathbf{v}(t)}{dt} = \mathbf{\gamma} \mathbf{A}\mathbf{v}(t).$$

In the above, $\mathbf{\gamma}$ is a diagonal matrix in which elements correspond to individual component response rates. Therefore, $\mathbf{M} = \mathbf{\gamma} \mathbf{A}$ defines the values of system components and can be analysed using the techniques of May [@May1972; @May1973; @Ahmadian2015]. In these analyses, row means of $\mathbf{A}$ are expected to be identical, but variation around this expectation will naturally arise due to random sampling of $\mathbf{A}$ off-diagonal elements and finite $S$. In simulations, the total variation in $\mathbf{M}$ row means that is attributable to $\mathbf{A}$ is small relative to that attributable to $\mathbf{\gamma}$, especially at high $S$. Variation in $\mathbf{\gamma}$ specifically isolates the effects of differing component response rates, hence causing differences in expected $\mathbf{M}$ row means.

<!--- Need a section here on how the structured networks were built --->

**Genetic algorithm**. Ideally, to investigate the potential of $Var(\gamma)$ for increasing the proportion of stable complex systems, the search space of all possible $\gamma$ vectors would be evaluated for each unique $\mathbf{M = \gamma A}$. This is technically impossible because $\gamma_{i}$ can take any real value between 0-2, but even rounding $\gamma_{i}$ to reasonable values would result in a search space too large to practically explore. Under these conditions, genetic algorithms are highly useful tools for finding practical solutions by mimicking the process of biological evolution [@Hamblin2013]. In this case, the practical solution is finding vectors of $\mathbf{\gamma}$ that decrease the most positive real eigenvalue of $\mathbf{M}$. The genetic algorithm used achieves this by initialising a large population of 1000 different potential $\mathbf{\gamma}$ vectors and allowing this population to evolve through a process of mutation, crossover (swaping $\gamma_{i}$ values between vectors), selection, and reproduction until either a $\mathbf{\gamma}$ vector is found where all $\Re(\lambda) < 0$ or some "giving up" critiera is met.

For each $S = \{2, 3, ..., 39, 40\}$, the genetic algorithm was run for 100000 random $\mathbf{M}$ ($\sigma = 0.4$, $C = 1$). The genetic algorithm was initialised with a population of 1000 different $\mathbf{\gamma}$ vectors with elements sampled i.i.d from $\gamma_{i} \sim \mathcal{U}(0, 2)$. Eigenanalysis was performed on the $\mathbf{M}$ resulting from each $\mathbf{\gamma}$ vector, and the 20 $\mathbf{\gamma}$ vectors resulting in $\mathbf{M}$ with the lowest $\max\left(\Re(\lambda)\right)$ each produced 50 clonal offspring with subsequent random mutation and crossover between the resulting new generation of 1000 $\mathbf{\gamma}$ vectors. Mutation of each $\gamma_{i}$ in a $\mathbf{\gamma}$ vector occurred with a probability of 0.2, resulting in a mutation effect of size $\mathcal{N}(0, 0.02)$ being added to generate the newly mutated $\gamma_{i}$ (any $\gamma_{i}$ values that mutated below zero were multiplied by $-1$, and any values that mutated above 2 were set to 2). Crossover occurred between two sets of 100 $\mathbf{\gamma}$ vectors paired in each generation; vectors were randomly sampled with replacement among but not within sets. Vector pairs selected for crossover swapped all elements between and including two $\gamma_{i}$ randomly selected with replacement (this allowed for reversal of vector element positions during crossover; e.g., $\{\gamma_{4}, \gamma_{5}, \gamma_{6}, \gamma_{7}\} \to \{\gamma_{7}, \gamma_{6}, \gamma_{5}, \gamma_{4}\}$ ). The genetic algorithm terminated if a stable $\mathbf{M}$ was found, 20 generations occurred, or if the mean $\mathbf{\gamma}$ fitness increase between generations was less than 0.01 (where fitness was defined as $W_{\gamma} = -\max\left(\Re(\lambda)\right)$ for $\mathbf{M}$).

**System feasibility**. Dougoud et al. [@Dougoud2018] identify the following feasibility criteria for ecological systems characterised by $S$ interacting species with varying densities in a generalised Lotka-Volterra model,

$$\mathbf{n^{*}} = -\left(\theta \mathbf{I} + (CS)^{-\delta}\mathbf{J} \right)^{-1}\mathbf{r}.$$

In the above, $\mathbf{n^{*}}$ is the vector of species densities at equilibrium. Feasibility is satisfied if all elements in $\mathbf{n^{*}}$ are positive. The matrix $\mathbf{I}$ is the identity matrix, and the value $\theta$ is the strength of intraspecific competition (diagonal elements). Diagonal values are set to $-1$, so $\theta = -1$. The variable $\delta$ is a normalisation parameter that modulates the strength of interactions ($\sigma$) for $\mathbf{J}$. Implicitly, here $\delta = 0$ underlying strong interactions. Hence, $(CS)^{-\delta} = 1$, so in the above, a diagonal matrix of -1s ($\theta \mathbf{I}$) is added to $\mathbf{J}$, which has a diagonal of all zeros and an off-diagonal affecting species interactions (i.e., the expression $(CS)^{-\delta}$ relates to May's [@May1972] stability criterion [@Dougoud2018] by $\frac{\sigma}{(CS)^{-\delta}}\sqrt{SC} < 1$, and hence for my purposes $(CS)^{-\delta} = 1$). Given $\mathbf{A} = \theta\mathbf{I + J}$, the above criteria is therefore reduced to the below (see also [@Servan2018]),

$$\mathbf{n^{*} = -A^{-1}r}.$$

To check the feasibility criteria for $\mathbf{M = \gamma A}$, I therefore evaluated $\mathbf{-M^{-1}r}$ ($\mathbf{r}$ elements were sampled i.i.d. from $r_{i} \sim \mathcal{N}(0, 0.4^{2})$). Feasibility is satisfied if all of the elements of the resulting vector are positive.

**Acknowledgements:** I am supported by a Leverhulme Trust Early Career Fellowship (ECF-2016-376). Conversations with L. Bussi&#xe8;re and N. Bunnefeld, and comments from J. J. Cusack and I. L. Jones, improved the quality of this work. 

**Supplementary Information:** Full tables of stability results for simulations across different system size ($S$) values, ecological community types, connectance ($C$) values, interaction strengths ($\sigma$), and $\gamma$ distributions are provided as supplementary material. An additional table also shows results for how feasibility changes across $S$. All code and simulation outputs are publicly available as part of the RandomMatrixStability package on GitHub (https://github.com/bradduthie/RandomMatrixStability).

**Additional Information:** The author declares no competing interests. All work was carried out by A. Bradley Duthie, and all code and data are accessible on [GitHub](https://github.com/bradduthie/RandomMatrixStability).

**References**

<div id="refs"></div>

\clearpage

```{r echo = FALSE}
S     <- 200;
C     <- 0.05;
sigma <- 0.4;
pr_st <- read.csv(file = "sim_results/bi_gamma/bi_pr_st.csv");
pr_st <- pr_st[,-1];
```

```{r, echo = FALSE}
A0 <- read.csv(file = "sim_results/bi_gamma/S200_A0.csv");
A0 <- as.matrix(A0[,-1]);
A1 <- read.csv(file = "sim_results/bi_gamma/S200_A1.csv");
A1 <- as.matrix(A1[,-1]);

A0_e   <- eigen(A0)$values;
A0_r   <- Re(A0_e);
A0_i   <- Im(A0_e);
A1_e   <- eigen(A1)$values;
A1_r   <- Re(A1_e);
A1_i   <- Im(A1_e);

A0_vm       <- A0;
diag(A0_vm) <- NA;
A0vec       <- as.vector(t(A0_vm));
A0vec       <- A0vec[is.na(A0vec) == FALSE];
A1_vm       <- A1;
diag(A1_vm) <- NA;
A1vec       <- as.vector(t(A1_vm));
A1vec       <- A1vec[is.na(A1vec) == FALSE];
fhalf       <- 1:(0.5*length(A1vec));
shalf       <- (0.5*length(A1vec)+1):length(A1vec);
pr_st       <- read.csv(file = "sim_results/bi_gamma/bi_pr_st.csv");
pr_st       <- pr_st[,-1];
```

**Figure 1: Example distribution of eigenvalues before (a) and after (b) separating a randomly generated complex system into fast ($\boldsymbol{\gamma} = 1.95$) and slow ($\boldsymbol{\gamma} = 0.05$) component response rates.** Each panel shows the same system where $S = 200$, $C = 0.05$, and $\sigma = 0.4$, and in each case $E[\gamma] = 1$ (i.e., only the distribution of $\gamma$ differs between panels).  **a.** Eigenvalues plotted when all $\gamma = 1$; distributions of points are uniformly distributed within the blue circle with a radius of $\sigma\sqrt{SC} =$ `r round(sqrt(200) * sd(A0vec), digits =3)` centred at -1 on the real axis. **b.** Eigenvalues plotted when half $\gamma = 1.95$ and half $\gamma = 0.05$; distributions of points can be partitioned into one large circle centred at $\gamma = -1.95$ and one small circle centred at $\gamma = -0.05$. In a, the maximum real eigenvalue $\max\left(\Re(\lambda)\right) =$ `r format(max(A0_r), scientific = FALSE)`, while in b $\max\left(\Re(\lambda)\right) =$ `r format(max(A1_r), scientific = FALSE)`, meaning that the complex system in b but not a is stable because in b $\max\left(\Re(\lambda)\right) < 0$. In 1 million randomly generated complex systems under the same parameter values, `r sum(pr_st[,1])` was stable when $\gamma = 1$ while `r sum(pr_st[,2])` were stable when $\gamma = \{1.95, 0.05\}$. Overall, complex systems that are separated into fast versus slow components tend to be more stable than otherwise identical systems with identical component response rates.

```{r, eval = TRUE, echo = FALSE, fig.height = 6, fig.width = 9}
par(mfrow = c(1, 2), mar = c(0.5, 0.5, 0.5, 0.5), oma = c(5, 5, 0, 0));
plot(A0_r, A0_i, xlim = c(-3.7, 0.3), ylim = c(-2, 2), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, 
     col = "dodgerblue4");
vl <- seq(from = 0, to = 2*pi, by = 0.001);
A0x0 <- sqrt(200) * sd(A0vec) * cos(vl) + mean(diag(A0));
A0y0 <- sqrt(200) * sd(A0vec) * sin(vl);
text(x = -3.5, y = 2.25, labels = "a", cex = 2);
points(x = A0x0, y = A0y0, type = "l", lwd = 3, col = "blue");
points(A0_r, A0_i, pch = 4, cex = 0.7, col = "dodgerblue4");

plot(A1_r, A1_i, xlim = c(-3.7, 0.3), ylim = c(-2, 2), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, 
     col = "firebrick", yaxt = "n");

vl <- seq(from = 0, to = 2*pi, by = 0.001);
A0x1a <- sqrt(100) * sd(A1vec[fhalf]) * cos(vl) + mean(diag(A1)[1:100]);
A0y1a <- sqrt(100) * sd(A1vec[fhalf]) * sin(vl);
points(x = A0x1a, y = A0y1a, type = "l", lwd = 3, col = "red");
A0x1b <- sqrt(100) * sd(A1vec[shalf]) * cos(vl) + mean(diag(A1)[101:200]);
A0y1b <- sqrt(100) * sd(A1vec[shalf]) * sin(vl);
points(x = A0x1b, y = A0y1b, type = "l", lwd = 3, col = "red");

points(A1_r[1:100], A1_i[1:100],pch = 4, cex = 0.7, col = "firebrick");   

text(x = -3.5, y = 2.25, labels = "b", cex = 2);
mtext(side = 1, "Real", outer = TRUE, line = 3, cex = 2);
mtext(side = 2, "Imaginary", outer = TRUE, line = 2.5, cex = 2);
```


\clearpage


```{r echo = FALSE}
S     <- 1000;
C     <- 1;
sigma <- 0.4;
```

```{r, echo = FALSE}
A_comp <- NULL;
A_dat  <- rnorm(n = 1000000, mean = 0, sd = 0.4);
A_mat  <- matrix(data = A_dat, nrow = 1000);
C_dat  <- rbinom(n = 1000 * 1000, size = 1, prob = 0.05);
C_mat  <- matrix(data = C_dat, nrow = 1000, ncol = 1000);
A_mat  <- A_mat * C_mat;
A_mat  <- species_interactions(A_mat, type = 0);
gammas <- runif(n = 1000, min = 0, max = 2);

mu_gam <- mean(gammas);
diag(A_mat) <- -1;
A1     <- gammas * A_mat;
A0     <- mu_gam * A_mat;
A0_e   <- eigen(A0)$values;
A0_r   <- Re(A0_e);
A0_i   <- Im(A0_e);
A1_e   <- eigen(A1)$values;
A1_r   <- Re(A1_e);
A1_i   <- Im(A1_e);

A0_vm       <- A0;
diag(A0_vm) <- NA;
A0vec       <- as.vector(A0_vm);
A0vec       <- A0vec[is.na(A0vec) == FALSE];
```

**Figure 2: Distributions of eigenvalues before (a) and after (b) introducing variation in component response rate ($\boldsymbol{\gamma}$) in complex systems.** Each panel show the same system where $S = 1000$, $C = 0.05$, and $\sigma = 0.4$. **a.** Eigenvalues plotted in the absence of $Var(\gamma)$ where $E[\gamma] = 1$, versus **b.** eigenvalues plotted given $\gamma \sim \mathcal{U}(0, 2)$, which increases the variance of interaction strengths ($\sigma^{2}$) but also creates a cluster of eigenvalues toward the distribution's centre (-1, 0). Blue elipses in both panels show the circle centred on the distribution in panel a. Proportions of $\Re(\lambda) < 0$ are `r sum(A0_r < 0) / length(A0_r)` and `r sum(A1_r < 0) / length(A1_r)` for a and b, respectively.

```{r, eval = TRUE, echo = FALSE, fig.height = 6, fig.width = 9}
par(mfrow = c(1, 2), mar = c(0.5, 0.5, 0.5, 0.5), oma = c(5, 5, 0, 0));
plot(A0_r, A0_i, xlim = c(-6, 2.25), ylim = c(-5,4.25), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, 
     col = "dodgerblue4");
vl <- seq(from = 0, to = 2*pi, by = 0.001);
x0 <- sqrt(1000) * sd(A0vec) * cos(vl) + mean(diag(A0));
y0 <- sqrt(1000) * sd(A0vec) * sin(vl);
text(x = -5.5, y = 4, labels = "a", cex = 2);
points(x = x0, y = y0, type = "l", lwd = 3, col = "blue");
points(A0_r, A0_i, col = "dodgerblue4",pch = 4, cex = 0.7);
plot(A1_r, A1_i, xlim = c(-6, 2.25), ylim = c(-5,4.25), pch = 4, cex = 0.7,
     xlab = "", ylab = "", cex.lab = 1.5, cex.axis = 1.5, asp = 1, 
     col = "firebrick", yaxt = "n");
text(x = -5.5, y = 4, labels = "b", cex = 2);
points(x = x0, y = y0, type = "l", lwd = 3, lty = "solid", col = "blue");
points(A1_r, A1_i, col = "firebrick", pch = 4, cex = 0.7);
mtext(side = 1, "Real", outer = TRUE, line = 3, cex = 2);
mtext(side = 2, "Imaginary", outer = TRUE, line = 2.5, cex = 2);
```

\clearpage


```{r, echo = FALSE}
dat <- read.csv(file = "sim_results/C_1/random_all.csv");
dat <- dat[,-1];
```

**Figure 3: Stability of large complex systems with and without variation in component response rate ($\boldsymbol{\gamma}$).** The $\ln$ number of systems that are stable across different system sizes ($S$, max $S=50$) given $C = 1$, and the proportion of systems in which variation in $\gamma$ is critical for system stability. For each $S$, 1 million complex systems are randomly generated. Stability of each complex system is tested given variation in $\gamma$ by randomly sampling $\gamma \sim \mathcal{U}(0, 2)$. Stability given $Var(\gamma)$ is then compared to stability in an otherwise identical system in which $\gamma = E[\mathcal{U}(0, 2)]$ for all components. Blue and red bars show the number of stable systems in the absence and presence of $Var(\gamma)$, respectively. The black line shows the proportion of systems that are stable when $Var(\gamma) > 0$, but would be unstable if $Var(\gamma) = 0$.


```{r, eval = TRUE, echo = FALSE, fig.width = 9, fig.height = 7}
dat <- read.csv(file = "sim_results/C_1/random_all.csv");
dat <- dat[,-1];
plot_stables(dat);
```

\clearpage

**Figure 4: Stability of large complex systems given $\boldsymbol{\gamma = 1}$ versus targeted $\boldsymbol{Var(\gamma)}$.** The $\ln$ number of systems that are stable across different system sizes ($S$, max $S=40$) for $C = 1$, and the proportion of systems wherein a targeted search of $\gamma$ values successfully resulted in system stability. For each $S$, 100000 complex systems are randomly generated. Stability of each complex system is tested given variation in $\gamma$ using a genetic algorithm to maximise the effect of $\gamma$ values on increasing stability, as compared to stability in an otherwise identical system in which $\gamma$ is the same for all components. Blue bars show the number of stable systems in the absence of component response rate variation, while red bars show the number of stable systems that can be generated if component response rate is varied to maximise system stability. The black line shows the proportion of systems that are stable when component response rate is targeted to increase stability, but would not be stable if $Var(\gamma) = 0$.

```{r, eval = TRUE, echo = FALSE, fig.width = 9, fig.height = 7}
dat <- read.csv(file = "sim_results/evolved/evo_results.csv");
dat <- dat[,-1];
plot_stable_4(dat);
```

